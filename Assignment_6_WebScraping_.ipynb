{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xemgomez/ElaineGomez_DTSC3020.020_Fall2025/blob/main/Assignment_6_WebScraping_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_de5Eq4u-tR"
      },
      "source": [
        "# Assignment 6 (4 points) — Web Scraping\n",
        "\n",
        "In this assignment you will complete **two questions**. The **deadline is posted on Canvas**.\n"
      ],
      "id": "H_de5Eq4u-tR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PHwamZMu-tX"
      },
      "source": [
        "## Assignment Guide (Read Me First)\n",
        "\n",
        "- This notebook provides an **Install Required Libraries** cell and a **Common Imports & Polite Headers** cell. Run them first.\n",
        "- Each question includes a **skeleton**. The skeleton is **not** a solution; it is a lightweight scaffold you may reuse.\n",
        "- Under each skeleton you will find a **“Write your answer here”** code cell. Implement your scraping, cleaning, and saving logic there.\n",
        "- When your code is complete, run the **Runner** cell to print a Top‑15 preview and save the CSV.\n",
        "- Expected outputs:\n",
        "  - **Q1:** `data_q1.csv` + Top‑15 sorted by the specified numeric column.\n",
        "  - **Q2:** `data_q2.csv` + Top‑15 sorted by `points`.\n"
      ],
      "id": "4PHwamZMu-tX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7DLq9nEu-tZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d1ad7a2-799c-4449-bdaf-0cfe2c3733f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies installed.\n"
          ]
        }
      ],
      "source": [
        " #Install Required Libraries\n",
        "!pip -q install requests beautifulsoup4 lxml pandas\n",
        "print(\"Dependencies installed.\")\n"
      ],
      "id": "I7DLq9nEu-tZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug_A9RuPu-tb"
      },
      "source": [
        "### 2) Common Imports & Polite Headers"
      ],
      "id": "ug_A9RuPu-tb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov8pXh65u-tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9cc185-a7d3-4194-a018-63eff015c03c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common helpers loaded.\n"
          ]
        }
      ],
      "source": [
        "# Common Imports & Polite Headers\n",
        "import re, sys, pandas as pd, requests\n",
        "from bs4 import BeautifulSoup\n",
        "HEADERS = {\"User-Agent\": (\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/122.0 Safari/537.36\")}\n",
        "def fetch_html(url: str, timeout: int = 20) -> str:\n",
        "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "def flatten_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [\" \".join([str(x) for x in tup if str(x)!=\"nan\"]).strip()\n",
        "                      for tup in df.columns.values]\n",
        "    else:\n",
        "        df.columns = [str(c).strip() for c in df.columns]\n",
        "    return df\n",
        "print(\"Common helpers loaded.\")\n"
      ],
      "id": "Ov8pXh65u-tc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km0GO7zzu-td"
      },
      "source": [
        "## Question 1 — IBAN Country Codes (table)\n",
        "**URL:** https://www.iban.com/country-codes  \n",
        "**Extract at least:** `Country`, `Alpha-2`, `Alpha-3`, `Numeric` (≥4 cols; you may add more)  \n",
        "**Clean:** trim spaces; `Alpha-2/Alpha-3` → **UPPERCASE**; `Numeric` → **int** (nullable OK)  \n",
        "**Output:** write **`data_q1.csv`** and **print a Top-15** sorted by `Numeric` (desc, no charts)  \n",
        "**Deliverables:** notebook + `data_q1.csv` + short `README.md` (URL, steps, 1 limitation)\n",
        "\n",
        "**Tip:** You can use `pandas.read_html(html)` to read tables and then pick one with ≥3 columns.\n"
      ],
      "id": "km0GO7zzu-td"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q1 Skeleton (fill the TODOs) ---\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Return the first table with >= 3 columns from the HTML.\n",
        "    TODO: implement with pd.read_html(html), pick a reasonable table, then flatten headers.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_read_table\")\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean columns: strip, UPPER Alpha-2/Alpha-3, cast Numeric to int (nullable), drop invalids.\n",
        "    TODO: implement cleaning steps.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_clean\")\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort descending by Numeric and return Top-N.\n",
        "    TODO: implement.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_sort_top\")\n"
      ],
      "id": "q1_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q1_skeleton_answer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faf250c7-1652-4b9c-a3d4-79b445346ff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote 15 rows to data_q1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2802630245.py:39: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  tables: List[pd.DataFrame] = pd.read_html(html, flavor=\"lxml\")\n"
          ]
        }
      ],
      "source": [
        "# Q1 — Write your answer here\n",
        "\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "from typing import List\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "URL = \"https://www.iban.com/country-codes\"\n",
        "OUTFILE = \"data_q1.csv\"\n",
        "\n",
        "\n",
        "# --- Helpers ---------------------------------------------------------------\n",
        "\n",
        "CONTROL_CHARS_RE = re.compile(r\"[\\x00-\\x1F\\x7F]\")  # control chars incl. \\n,\\r,\\t\n",
        "MULTISPACE_RE = re.compile(r\"\\s+\")\n",
        "\n",
        "def _clean_text(val: str) -> str:\n",
        "    \"\"\"Unicode-normalize, drop control chars/zero-widths, collapse spaces.\"\"\"\n",
        "    if pd.isna(val):\n",
        "        return \"\"\n",
        "    s = str(val)\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = s.replace(\"\\u200b\", \"\").replace(\"\\ufeff\", \"\")  # zero-width/BOM\n",
        "    s = CONTROL_CHARS_RE.sub(\" \", s)\n",
        "    s = MULTISPACE_RE.sub(\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "\n",
        "# --- Q1 Skeleton (fill the TODOs) ---\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Return the first table with >= 3 columns from the HTML.\n",
        "    TODO: implement with pd.read_html(html), pick a reasonable table, then flatten headers.\n",
        "    \"\"\"\n",
        "    # Use pandas to parse all HTML tables\n",
        "    tables: List[pd.DataFrame] = pd.read_html(html, flavor=\"lxml\")\n",
        "    if not tables:\n",
        "        raise ValueError(\"No tables found on the page.\")\n",
        "\n",
        "    # Pick the first table with >= 3 columns\n",
        "    for df in tables:\n",
        "        if df.shape[1] >= 3:\n",
        "            # Flatten headers (handles MultiIndex or messy headers)\n",
        "            if isinstance(df.columns, pd.MultiIndex):\n",
        "                df.columns = [\n",
        "                    \" \".join([_clean_text(x) for x in tup if pd.notna(x)]).strip()\n",
        "                    for tup in df.columns.to_list()\n",
        "                ]\n",
        "            else:\n",
        "                df.columns = [_clean_text(c) for c in df.columns]\n",
        "            return df\n",
        "\n",
        "    raise ValueError(\"No table with >= 3 columns found.\")\n",
        "\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean columns: strip, UPPER Alpha-2/Alpha-3, cast Numeric to int (nullable), drop invalids.\n",
        "    TODO: implement cleaning steps.\n",
        "    \"\"\"\n",
        "    # Try to detect canonical columns by name (case/spacing tolerant)\n",
        "    colmap = {}\n",
        "    for c in df.columns:\n",
        "        lc = c.lower()\n",
        "        if \"country\" in lc:\n",
        "            colmap[c] = \"country\"\n",
        "        elif \"alpha-2\" in lc or \"alpha 2\" in lc:\n",
        "            colmap[c] = \"alpha_2\"\n",
        "        elif \"alpha-3\" in lc or \"alpha 3\" in lc:\n",
        "            colmap[c] = \"alpha_3\"\n",
        "        elif \"numeric\" in lc:\n",
        "            colmap[c] = \"numeric\"\n",
        "\n",
        "    df = df.rename(columns=colmap)\n",
        "    # Keep only the needed columns if present\n",
        "    needed = [\"country\", \"alpha_2\", \"alpha_3\", \"numeric\"]\n",
        "    present = [c for c in needed if c in df.columns]\n",
        "    if len(present) < 4:\n",
        "        raise ValueError(f\"Expected columns not found. Got: {df.columns.tolist()}\")\n",
        "\n",
        "    df = df[present].copy()\n",
        "\n",
        "    # Clean text columns\n",
        "    for col in [\"country\", \"alpha_2\", \"alpha_3\"]:\n",
        "        df[col] = df[col].astype(str).map(_clean_text)\n",
        "\n",
        "    # Uppercase alpha codes\n",
        "    df[\"alpha_2\"] = df[\"alpha_2\"].str.upper()\n",
        "    df[\"alpha_3\"] = df[\"alpha_3\"].str.upper()\n",
        "\n",
        "    # Clean numeric: keep digits only, then cast to nullable Int64\n",
        "    df[\"numeric\"] = (\n",
        "        df[\"numeric\"]\n",
        "        .astype(str)\n",
        "        .map(lambda s: re.sub(r\"\\D\", \"\", s))\n",
        "        .replace({\"\": pd.NA})\n",
        "    )\n",
        "    df[\"numeric\"] = pd.to_numeric(df[\"numeric\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "    # Drop invalids: require non-empty country; alpha_2 len==2; alpha_3 len==3; numeric not NA\n",
        "    df = df[\n",
        "        (df[\"country\"].str.len() > 0)\n",
        "        & (df[\"alpha_2\"].str.len() == 2)\n",
        "        & (df[\"alpha_3\"].str.len() == 3)\n",
        "        & (df[\"numeric\"].notna())\n",
        "    ].copy()\n",
        "\n",
        "    # Final whitespace polish\n",
        "    for col in [\"country\", \"alpha_2\", \"alpha_3\"]:\n",
        "        df[col] = df[col].str.strip()\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort descending by Numeric and return Top-N.\n",
        "    TODO: implement.\n",
        "    \"\"\"\n",
        "    if \"numeric\" not in df.columns:\n",
        "        raise ValueError(\"Column 'numeric' not found for sorting.\")\n",
        "    out = df.sort_values(\"numeric\", ascending=False, na_position=\"last\").head(top)\n",
        "    return out.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# --- Main script -----------------------------------------------------------\n",
        "\n",
        "def main():\n",
        "    # Fetch HTML with a reasonable UA and encoding handling\n",
        "    headers = {\n",
        "        \"User-Agent\": (\n",
        "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "            \"Chrome/120.0.0.0 Safari/537.36\"\n",
        "        )\n",
        "    }\n",
        "    r = requests.get(URL, headers=headers, timeout=30)\n",
        "    if not r.encoding:\n",
        "        r.encoding = r.apparent_encoding or \"utf-8\"\n",
        "    html = r.text\n",
        "\n",
        "    # Pipeline: read -> clean -> sort -> top 15\n",
        "    raw_df = q1_read_table(html)\n",
        "    clean_df = q1_clean(raw_df)\n",
        "    top_df = q1_sort_top(clean_df, top=15)\n",
        "\n",
        "    # Write CSV (UTF-8 with BOM for Excel friendliness)\n",
        "    top_df.to_csv(OUTFILE, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"Wrote {len(top_df)} rows to {OUTFILE}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "id": "q1_skeleton_answer"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmefu--_u-tg"
      },
      "source": [
        "## Question 2 — Hacker News (front page)\n",
        "**URL:** https://news.ycombinator.com/  \n",
        "**Extract at least:** `rank`, `title`, `link`, `points`, `comments` (user optional)  \n",
        "**Clean:** cast `points`/`comments`/`rank` → **int** (non-digits → 0), fill missing text fields  \n",
        "**Output:** write **`data_q2.csv`** and **print a Top-15** sorted by `points` (desc, no charts)  \n",
        "**Tip:** Each story is a `.athing` row; details (points/comments/user) are in the next `<tr>` with `.subtext`.\n"
      ],
      "id": "rmefu--_u-tg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q2 Skeleton (fill the TODOs) ---\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse front page items into DataFrame columns:\n",
        "       rank, title, link, points, comments, user (optional).\n",
        "    TODO: implement with BeautifulSoup on '.athing' and its sibling '.subtext'.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_parse_items\")\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\n",
        "    TODO: cast points/comments/rank to int (non-digits -> 0). Fill text fields.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_clean\")\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N. TODO: implement.\"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_sort_top\")\n"
      ],
      "id": "q2_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q2_skeleton_answer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d1ab16-8344-4121-c9aa-fcf371dab45a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 15 Hacker News Stories:\n",
            "\n",
            " 1. (894 pts, 353 comments)\n",
            "    You should write an agent\n",
            "    Link: https://fly.io/blog/everyone-write-an-agent/\n",
            "\n",
            " 2. (564 pts, 128 comments)\n",
            "    Leaving Meta and PyTorch\n",
            "    Link: https://soumith.ch/blog/2025-11-06-leaving-meta-and-pytorch.md.html\n",
            "\n",
            " 3. (561 pts, 389 comments)\n",
            "    Two billion email addresses were exposed\n",
            "    Link: https://www.troyhunt.com/2-billion-email-addresses-were-exposed-and-we-indexed-them-all-in-have-i-been-pwned/\n",
            "\n",
            " 4. (513 pts, 209 comments)\n",
            "    Show HN: I scraped 3B Goodreads reviews to train a better recommendation model\n",
            "    Link: https://book.sv\n",
            "\n",
            " 5. (455 pts, 157 comments)\n",
            "    A Fond Farewell\n",
            "    Link: https://www.farmersalmanac.com/fond-farewell-from-farmers-almanac\n",
            "\n",
            " 6. (430 pts, 136 comments)\n",
            "    Game design is simple\n",
            "    Link: https://www.raphkoster.com/2025/11/03/game-design-is-simple-actually/\n",
            "\n",
            " 7. (402 pts, 294 comments)\n",
            "    Meta projected 10% of 2024 revenue came from scams\n",
            "    Link: https://sherwood.news/tech/meta-projected-10-of-2024-revenue-came-from-scams-and-banned-goods-reuters/\n",
            "\n",
            " 8. (223 pts, 180 comments)\n",
            "    Analysis indicates that the universe’s expansion is not accelerating\n",
            "    Link: https://ras.ac.uk/news-and-press/research-highlights/universes-expansion-now-slowing-not-speeding\n",
            "\n",
            " 9. (153 pts, 51 comments)\n",
            "    OpenMW 0.50.0 Released – open-source Morrowind reimplementation\n",
            "    Link: https://openmw.org/2025/openmw-0-50-0-released/\n",
            "\n",
            "10. (139 pts, 52 comments)\n",
            "    From web developer to database developer in 10 years\n",
            "    Link: https://notes.eatonphil.com/2025-02-15-from-web-developer-to-database-developer-in-10-years.html\n",
            "\n",
            "11. (118 pts, 35 comments)\n",
            "    Text case changes the size of QR codes\n",
            "    Link: https://www.johndcook.com/blog/2025/10/31/smaller-qr-codes/\n",
            "\n",
            "12. (113 pts, 49 comments)\n",
            "    I Love OCaml\n",
            "    Link: https://mccd.space/posts/ocaml-the-worlds-best/\n",
            "\n",
            "13. (97 pts, 84 comments)\n",
            "    We chose OCaml to write Stategraph\n",
            "    Link: https://stategraph.dev/blog/why-we-chose-ocaml\n",
            "\n",
            "14. (90 pts, 19 comments)\n",
            "    PyTorch Helion\n",
            "    Link: https://pytorch.org/blog/helion/\n",
            "\n",
            "15. (72 pts, 128 comments)\n",
            "    Is Software the UFOlogy of Engineering Disciplines?\n",
            "    Link: https://codemanship.wordpress.com/2025/11/07/is-software-the-ufology-of-engineering-disciplines/\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Q2 — Write your answer here\n",
        "\n",
        "\n",
        "    #!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import html\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "HN_URL = \"https://news.ycombinator.com/\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (compatible; HN-Q2-Scraper/1.1; +https://news.ycombinator.com/)\"\n",
        "}\n",
        "\n",
        "\n",
        "def _fetch(url: str, retries: int = 3, backoff: float = 1.5) -> str:\n",
        "    last = None\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            resp = requests.get(url, headers=HEADERS, timeout=15)\n",
        "            resp.raise_for_status()\n",
        "            resp.encoding = resp.apparent_encoding or \"utf-8\"\n",
        "            return resp.text\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            time.sleep(backoff ** (i + 1))\n",
        "    raise RuntimeError(f\"Failed to fetch {url}: {last}\")\n",
        "\n",
        "\n",
        "def _clean_text(s: str) -> str:\n",
        "    \"\"\"Normalize & remove undesirable/control chars.\"\"\"\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    s = html.unescape(s)\n",
        "    s = s.replace(\"\\xa0\", \" \")\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = \"\".join(ch for ch in s if unicodedata.category(ch)[0] != \"C\")\n",
        "    return s.strip()\n",
        "\n",
        "\n",
        "def _to_int_or_zero(text: str) -> int:\n",
        "    if not text:\n",
        "        return 0\n",
        "    m = re.search(r\"\\d+\", text.replace(\",\", \"\"))\n",
        "    return int(m.group()) if m else 0\n",
        "\n",
        "\n",
        "# --- Q2 Skeleton (filled) ---\n",
        "def q2_parse_items(html_text: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse front page items into DataFrame columns:\n",
        "       rank, title, link, points, comments, user (optional).\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "\n",
        "    for athing in soup.select(\"tr.athing\"):\n",
        "        # Title & link\n",
        "        title_a = athing.select_one(\"span.titleline > a\")\n",
        "        title = _clean_text(title_a.get_text(strip=True)) if title_a else \"\"\n",
        "        link = title_a[\"href\"].strip() if title_a and title_a.has_attr(\"href\") else \"\"\n",
        "\n",
        "        # Rank like \"1.\"\n",
        "        rank_txt = _clean_text((athing.select_one(\"span.rank\") or {}).get_text() if athing else \"\")\n",
        "        # subtext row (points, user, age, comments) is the next <tr>\n",
        "        subtext = None\n",
        "        nxt = athing.find_next_sibling(\"tr\")\n",
        "        if nxt:\n",
        "            subtext = nxt.select_one(\"td.subtext\")\n",
        "\n",
        "        # Points like \"123 points\"\n",
        "        points_txt = \"\"\n",
        "        if subtext:\n",
        "            score = subtext.select_one(\"span.score\")\n",
        "            points_txt = _clean_text(score.get_text()) if score else \"\"\n",
        "\n",
        "        # Comments and user\n",
        "        comments_txt = \"\"\n",
        "        user_txt = \"\"\n",
        "        if subtext:\n",
        "            user_a = subtext.select_one(\"a.hnuser\")\n",
        "            user_txt = _clean_text(user_a.get_text()) if user_a else \"\"\n",
        "            anchors = subtext.find_all(\"a\")\n",
        "            if anchors:\n",
        "                last_a_txt = _clean_text(anchors[-1].get_text())\n",
        "                if \"comment\" in last_a_txt.lower():\n",
        "                    comments_txt = last_a_txt\n",
        "\n",
        "        rows.append(\n",
        "            {\n",
        "                \"rank\": rank_txt,\n",
        "                \"title\": title,\n",
        "                \"link\": link,\n",
        "                \"points\": points_txt,\n",
        "                \"comments\": comments_txt,\n",
        "                \"user\": user_txt,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\"\"\"\n",
        "    out = df.copy()\n",
        "\n",
        "    # Ensure expected columns exist\n",
        "    for col in [\"rank\", \"title\", \"link\", \"points\", \"comments\", \"user\"]:\n",
        "        if col not in out.columns:\n",
        "            out[col] = \"\"\n",
        "\n",
        "    # Clean/normalize text fields\n",
        "    for col in [\"title\", \"link\", \"user\"]:\n",
        "        out[col] = out[col].astype(str).map(_clean_text).fillna(\"\")\n",
        "\n",
        "    # Cast numerics (non-digits -> 0)\n",
        "    out[\"rank\"] = out[\"rank\"].astype(str).map(_to_int_or_zero)\n",
        "    out[\"points\"] = out[\"points\"].astype(str).map(_to_int_or_zero)\n",
        "    out[\"comments\"] = out[\"comments\"].astype(str).map(_to_int_or_zero)\n",
        "\n",
        "    # Make sure dtypes are plain ints\n",
        "    out[\"rank\"] = out[\"rank\"].astype(int)\n",
        "    out[\"points\"] = out[\"points\"].astype(int)\n",
        "    out[\"comments\"] = out[\"comments\"].astype(int)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N.\"\"\"\n",
        "    return (\n",
        "        df.sort_values([\"points\", \"rank\"], ascending=[False, True])\n",
        "          .head(top)\n",
        "          .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "\n",
        "# --- Runner ---\n",
        "def main():\n",
        "    html_text = _fetch(HN_URL)\n",
        "    df_raw = q2_parse_items(html_text)\n",
        "    df = q2_clean(df_raw)\n",
        "    top15 = q2_sort_top(df, top=15)\n",
        "\n",
        "    # Save CSV with safe encoding\n",
        "    top15.to_csv(\"data_q2.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    # Pretty console output\n",
        "    print(\"\\nTop 15 Hacker News Stories:\\n\")\n",
        "    for i, row in top15.iterrows():\n",
        "        print(f\"{i+1:>2}. ({row['points']} pts, {row['comments']} comments)\")\n",
        "        print(f\"    {row['title']}\")\n",
        "        print(f\"    Link: {row['link']}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "id": "q2_skeleton_answer"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}